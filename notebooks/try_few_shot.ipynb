{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot semantic segmentation for PV suitability assessment\n",
    "\n",
    "### Memo\n",
    "\n",
    "\n",
    "This notebook offers an in-depth look at few-shot learning (FSL), an advanced deep learning technique that enables models to generalize effectively from only a small number of training examples. FSL becomes particularly valuable when traditional supervised learning is impractical. For instance, when annotation is expensive, requires specialized expertise, or when data is scarce.  \n",
    "Few-shot learning is especially relevant in the context of public policy, where we often face limited access to comprehensive, well-curated datasets or lack the resources to collect large amounts of training data. Applications span a wide range of policy areas, including disaster classification (Lee et al., 2025), urban planning (Hu et al., 2022) and health policy, where FSL has supported COVID-19 detection (Jadon, 2021) and the diagnosis of rare genetic diseases (Alsentzer et al., 2025).  \n",
    "Beyond classification tasks, FSL also performs well in segmentation problems, helping identify different types of buildings, vegetation such as forest cover (Puthumanaillam & Verma, 2023) and performing well in remote sensing imagery segmentation (Chen et al., 2022).All of these examples share a common challenge: limited data. Few-shot learning helps address this constraint, and we encounter similar situations within our own institution. A clear illustration is our Wildlife Trade Monitoring Project (WTMP), where labeled examples of rare, endangered species are extremely scarce and often inconsistent. This emphasizes just how valuable few-shot learning approaches can be.  \n",
    "Today, a wide range of few-shot learning frameworks and architectures are available. Some of the most widely used include model-agnostic meta-learning (Finn, Abbeel & Levine, 2017), prototypical networks (Snell, Swersky & Zemel, 2017), and relation networks (Sung et al., 2018). Over the past few years, the field has continued to evolve, giving rise to more specialized and state-of-the-art techniques, such as SegPPD-FS for semantic segmentation (Ge et al., 2025) and CDCNet for classification tasks (Li et al., 2025).   \n",
    "Because of its importance to public policy and its potential value for our organization, this notebook takes a deep dive into prototypical networks semantic segmentation of building rooftops. This is an essential step for many downstream tasks, such as assessing rooftop suitability for solar panel installation. By walking through this method, the notebook aims to help our staff better understand and apply this powerful technique in their day-to-day work.\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Dear colleagues,  \n",
    "In this notebook, we will introduce the concept of few-shot learning (FSL). As briefly mentioned earlier, FSL is an advanced deep learning paradigm that enables models to generalize from only a handful of annotated samples. In particular, we will focus on prototypical networks, which learn a metric space where each class is represented by a prototype (usually the mean embedding of its support examples) and then classify query samples based on their distance to these prototypes (more details will be provided below). This method can be applied to semantic segmentation tasks, allowing us to partition an image into meaningful regions.\n",
    "To guide you through this approach, we will work with the Geneva Satellite Images dataset. This dataset contains high-resolution (250×250 PNG tiles) satellite imagery of Geneva, Switzerland, along with pixel-level segmentation masks that delineate building rooftops. It was developed by EPFL’s Solar Energy and Building Physics Laboratory and is publicly available on [HuggingFace](https://huggingface.co/datasets/raphaelattias/overfitteam-geneva-satellite-images)\n",
    " (Castello et al., 2021).  \n",
    "The goal of this notebook is to demonstrate how few-shot learning can be used for rooftop semantic segmentation, with applications such as assessing the suitability of buildings for solar panels.  \n",
    "\n",
    "Throughout the notebook, we will walk you through how to:  \n",
    "\n",
    "- Load and preprocess satellite imagery\n",
    "- Build a few-shot segmentation model\n",
    "- Train the model using prototypical networks\n",
    "- Evaluate segmentation performance  \n",
    "  \n",
    "Your task is to follow the code and understand how to apply few-shot learning to detect rooftops in new satellite images using only a few labeled examples. You can run each cell in sequence. At every step, we provide clear explanations of what is happening. A pre-recorded video is also available if you prefer a more visual learning format.  \n",
    "\n",
    "If you have any questions about few-shot learning or need help with its implementation, please feel free to reach out. Our contact information is provided above.  \n",
    " \n",
    "Happy learning!  \n",
    "\n",
    "Your Public Innovation Lab (PILAB)  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Task description\n",
    "\n",
    "\n",
    "- **Binary semantic segmentation** problem: for each pixel in a satellite tile, we predict whether it is part of a rooftop suitable for photovoltaics (PV) or background.\n",
    "- The method is based on **prototypical networks for segmentation**: we learn an encoder that maps pixels into an embedding space and then classify each pixel by its distance to simple “prototypes” built from a small number of labeled support images. \n",
    "\n",
    "Instead of training a standard segmentation network on a fully labeled dataset, we:\n",
    "\n",
    "- Meta-train an encoder on “episodes” that takes 1 support image + mask → learn prototypes → segment a query image.\n",
    "- At test time we can feed the model K labeled examples (K-shot) for a new set of tiles and segment unseen images with limited supervision.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook structure\n",
    "\n",
    "1. **Setup**: install packages and import dependencies  \n",
    "2. **Data download**: fetch the Geneva rooftop dataset from Hugging Face  \n",
    "3. **Base dataset**: load images and binary rooftop masks with transforms  \n",
    "4. **Visualizaion**: visualise a sample image + mask  \n",
    "5. **Few-shot episodes**: build an episodic dataset (support + query pairs)  \n",
    "6. **Encoder**: ResNet-based convolutional feature extractor  \n",
    "7. **Prototypes & classification**: compute foreground/background prototypes and classify pixels  \n",
    "8. **Metric**: define Intersection-over-Union (IoU) for evaluation  \n",
    "9. **Meta-training**: episodic training loop for the encoder  \n",
    "10. **Inference & evaluation**: K-shot prediction, IoU evaluation, and visualisations\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We first install and import the libraries needed for the tutorial:\n",
    "\n",
    "- `torch` and `torchvision` for deep learning and pretrained vision models,\n",
    "- `matplotlib`, `PIL`, and `numpy` for visualisation and array manipulation,\n",
    "- `huggingface_hub` to download the rooftop dataset from the Hugging Face Hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub torch torchvision matplotlib\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data download: Geneva rooftop tiles\n",
    "\n",
    "We use a small satellite dataset of building rooftops in Geneva, stored on the Hugging Face Hub. Each tile comes with:\n",
    "\n",
    "- a **RGB satellite image**, and\n",
    "- a **binary mask** indicating pixels that belong to rooftops suitable for PV installations.\n",
    "\n",
    "Using `huggingface_hub.snapshot_download`, we download the dataset to the local filesystem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = snapshot_download(repo_id=\"raphaelattias/overfitteam-geneva-satellite-images\", repo_type=\"dataset\")\n",
    "print(\"Dataset root:\", dataset_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing\n",
    "\n",
    "To work with PyTorch, we define a `Dataset` class that returns `(image, mask)` pairs:\n",
    "\n",
    "- Images are resized to a fixed resolution of **256 × 256**, converted to PyTorch tensors, and normalised with **ImageNet mean and standard deviation**. This is compatible with the pretrained ResNet18 backbone we will use later. \n",
    "- Masks are resized using **nearest-neighbour interpolation** to avoid mixing labels at boundaries, converted to tensors in `[0, 1]`, and then **binarised** pixels so that any non-zero value is treated as rooftop (class 1): mask pixels can be either background or rooftop.\n",
    "\n",
    "The `GenevaRooftopDataset` class wraps these steps and reads from a structured folder layout with `images/` and `labels/` subdirectories. We instantiate three splits: train, validation, and test. These base datasets will later feed into both standard visualisation and our few-shot episodic sampling.\n",
    "\n",
    "> **Notice**  \n",
    "> *To be changed if we do geosplits*  \n",
    "> *Also, even if we don't do geosplits, we don't need the validation set*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE IF WE WANT TO SPLIT DATASET WITH THE GEOSPLIT (normal split below) - ONE HAS TO BE ERASED\n",
    "\n",
    "IMAGE_SIZE = 256  # resize tiles to this\n",
    "\n",
    "img_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mask_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation=Image.NEAREST),\n",
    "        transforms.ToTensor(),  # gives float [0,1] for grayscale\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def get_grid_id_from_filename(fname):\n",
    "    parts = fname.split(\"_\")\n",
    "    # 3th and 3th parts give the grid ID\n",
    "    return f\"{parts[2]}_{parts[3]}\"\n",
    "\n",
    "\n",
    "class GenevaRooftopDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset filtered by geographic grid IDs.\n",
    "    Can read from multiple splits (train/val/test) at once.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, splits=[\"train\", \"val\", \"test\"], category=\"all\", grid_ids=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.category = category\n",
    "        self.grid_ids = grid_ids\n",
    "        self.files_info = []  # list of tuples (split, filename)\n",
    "\n",
    "        # Collect files from all specified splits\n",
    "        for split in splits:\n",
    "            image_dir = os.path.join(root, split, \"images\", category)\n",
    "            label_dir = os.path.join(root, split, \"labels\", category)\n",
    "\n",
    "            all_files = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "\n",
    "            # Filter by grid_ids if provided\n",
    "            if grid_ids is not None:\n",
    "                all_files = [f for f in all_files if get_grid_id_from_filename(f) in grid_ids]\n",
    "\n",
    "            # Store split info for loading\n",
    "            self.files_info.extend([(split, f) for f in all_files])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        split, fname = self.files_info[idx]\n",
    "        img_path = os.path.join(self.root, split, \"images\", self.category, fname)\n",
    "        mask_name = fname.replace(\".png\", \"_label.png\")\n",
    "        mask_path = os.path.join(self.root, split, \"labels\", self.category, mask_name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        img = img_transform(img)\n",
    "        mask = mask_transform(mask)\n",
    "        mask = (mask > 0.5).float()\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "train_grids = [\"1301_11\", \"1301_31\"]\n",
    "test_grids = [\"1301_13\"]\n",
    "\n",
    "# Train dataset reads from all three folders\n",
    "train_base = GenevaRooftopDataset(dataset_root, splits=[\"train\", \"val\", \"test\"], grid_ids=train_grids)\n",
    "\n",
    "# Test dataset can read from just one folder or multiple if needed\n",
    "test_base = GenevaRooftopDataset(dataset_root, splits=[\"train\", \"val\", \"test\"], grid_ids=test_grids)\n",
    "\n",
    "print(f\"Train samples: {len(train_base)}, Test samples: {len(test_base)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization: image and mask\n",
    "\n",
    "We verify that images and masks line up correctly, and pixel masks have the expected binary structure. The helper function below randomly draws a sample from a given dataset, undoes the image normalisation for plotting, and displays the RGB image side-by-side with its rooftop mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(dataset, idx=None):\n",
    "    if idx is None:\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "    img, mask = dataset[idx]  # img [3,H,W], mask [1,H,W]\n",
    "\n",
    "    # Undo normalisation for plotting\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    # mask: [1, H, W] -> [H, W]\n",
    "    mask_np = mask.squeeze(0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_np)\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask_np, cmap=\"gray\")\n",
    "    plt.title(\"Mask (rooftop)\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_sample(train_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Methodology  \n",
    "\n",
    "## 5.1. Few-shot episodes: support and query\n",
    "\n",
    "Few-shot learning is typically implemented with **episodic training**. An **episode** is one small training problem that mimics how we will use the model at test time: we see some labelled examples (support) and we must make predictions for new data (query).\n",
    "\n",
    "In this setting, each episode contains:\n",
    "\n",
    "- a **support image with its mask** `(img_s, mask_s)`, and  \n",
    "- a **query image with its mask** `(img_q, mask_q)`.\n",
    "\n",
    "We create an `EpisodeDataset` that wraps the base training dataset. For every episode, it randomly picks two *different* tiles from the training set:\n",
    "\n",
    "- one becomes the support pair,\n",
    "- the other becomes the query pair.\n",
    "\n",
    "During training, the model:\n",
    "\n",
    "1. looks at the **support image + mask** to learn what rooftops and background look like, and  \n",
    "2. tries to correctly segment the **query image**, using that information.\n",
    "\n",
    "The parameter `episodes_per_epoch` defines how many such support–query pairs (episodes) we use in one training epoch. A `DataLoader` iterates over this dataset and gives us **one episode at a time** for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Yields (support_imgs [K,3,H,W], support_masks [K,1,H,W], query_img, query_mask)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_dataset, episodes_per_epoch=750, K=1):\n",
    "        self.base = base_dataset\n",
    "        self.episodes_per_epoch = episodes_per_epoch\n",
    "        self.n = len(base_dataset)\n",
    "        self.K = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.episodes_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = random.sample(range(self.n), K + 1)\n",
    "        support_idx, query_idx = indices[:-1], indices[-1]\n",
    "\n",
    "        imgs_s, masks_s = [], []\n",
    "        for i in support_idx:\n",
    "            img_s, mask_s = self.base[i]\n",
    "            imgs_s.append(img_s)\n",
    "            masks_s.append(mask_s)\n",
    "\n",
    "        img_q, mask_q = self.base[query_idx]\n",
    "\n",
    "        imgs_s = torch.stack(imgs_s, dim=0)  # [K,3,H,W]\n",
    "        masks_s = torch.stack(masks_s, dim=0)  # [K,1,H,W]\n",
    "\n",
    "        return imgs_s, masks_s, img_q, mask_q\n",
    "\n",
    "\n",
    "episodes_per_epoch = 2000\n",
    "episode_dataset = EpisodeDataset(train_base, episodes_per_epoch=episodes_per_epoch)\n",
    "episode_loader = DataLoader(episode_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Encoder backbone: ResNet18 feature extractor\n",
    "\n",
    "We now define the feature extractor that underpins our prototypical network. The `Encoder` class:\n",
    "\n",
    "- uses a **pretrained ResNet18** from `torchvision`,\n",
    "- keeps the convolutional layers up to `layer3`, which downsample the input by a factor of 8, and\n",
    "- adds a `1×1` convolution to project the ResNet features into a configurable embedding dimension (here 256 channels).\n",
    "\n",
    "Given an input image of shape `[3, H, W]`, the encoder outputs a feature map `[C, H', W']`. These feature maps are the basis for computing class prototypes (rooftop vs background) and for classifying each pixel at test time.\n",
    "\n",
    "> **Notice:**\n",
    "> ResNet18 is lightweight and pretrained on ImageNet. We expect a model pretrained on geospatial/satellite images to perform better as feature extractor &rarr; can be a good *extension*.  \n",
    "> *Question: How changing the backbone architecture changes the performance?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Convolutional encoder using ResNet18 up to layer3 (downsampling by 8),\n",
    "#     followed by a 1x1 conv to get a compact embedding.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, out_channels=256, pretrained=True):\n",
    "#         super().__init__()\n",
    "#         backbone = models.resnet18(pretrained=pretrained)\n",
    "#         self.features = nn.Sequential(\n",
    "#             backbone.conv1,\n",
    "#             backbone.bn1,\n",
    "#             backbone.relu,\n",
    "#             backbone.maxpool,\n",
    "#             backbone.layer1,\n",
    "#             backbone.layer2,\n",
    "#             backbone.layer3,\n",
    "#         )\n",
    "#         # resnet18.layer3 output has 256 channels\n",
    "#         self.proj = nn.Conv2d(256, out_channels, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         f = self.features(x)    # [B, 256, H', W']\n",
    "#         f = self.proj(f)        # [B, C,   H', W']\n",
    "#         return f\n",
    "\n",
    "# encoder = Encoder(out_channels=256, pretrained=True).to(device)\n",
    "# print(encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchgeo.models import resnet18, ResNet18_Weights\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, out_channels=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet18(pretrained=pretrained)\n",
    "        self.stem = nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu,\n",
    "            backbone.maxpool,\n",
    "        )\n",
    "        self.layer1 = backbone.layer1\n",
    "        self.layer2 = backbone.layer2\n",
    "        self.layer3 = backbone.layer3\n",
    "\n",
    "        self.proj = nn.Conv2d(128 + 256, out_channels, kernel_size=1)  # 128 from layer2, 256 from layer3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        f1 = self.layer1(x)\n",
    "        f2 = self.layer2(f1)  # [B,128,H/4,W/4]\n",
    "        f3 = self.layer3(f2)  # [B,256,H/8,W/8]\n",
    "\n",
    "        f2_up = F.interpolate(f2, size=f3.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        f = torch.cat([f2_up, f3], dim=1)  # [B,384,H',W']\n",
    "        f = self.proj(f)\n",
    "        return f\n",
    "\n",
    "\n",
    "encoder = Encoder(out_channels=256).to(device)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Prototypical segmentation: prototypes and query classification\n",
    "\n",
    "The core idea of prototypical networks is to represent each class by a **prototype vector** in feature space and to classify new examples based on their distance to these prototypes.\n",
    "\n",
    "For segmentation we apply this idea at the **pixel level**. In this case we have only one class (we are segmenting rooftops from background): binary case (rooftop vs non-rooftop), therefore we are using a 2-class prototypical network.\n",
    "\n",
    "- From the support images and their masks, we compute:\n",
    "  - a **foreground prototype** (rooftops),\n",
    "  - and a **background prototype**,\n",
    "  by performing masked average pooling over the encoder feature maps.\n",
    "  \n",
    "- For a query image, we run the encoder once to obtain features and then:\n",
    "  - compute the squared Euclidean distance from each pixel’s feature vector to each prototype,\n",
    "  - convert distances into logits (negative distances),\n",
    "  - and reshape to a `[1, 2, H', W']` logit map over classes 0 (background) and 1 (rooftop).\n",
    "\n",
    "These two functions, `compute_prototypes` and `classify_query`, implement the prototypical segmentation logic used in both training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Prototype computation & query classification\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def compute_prototypes(feat_support, mask_support):\n",
    "    \"\"\"\n",
    "    Compute background/foreground prototypes from multiple support images.\n",
    "\n",
    "    feat_support: [K, C, H', W']   (K support images)\n",
    "    mask_support: [K, 1, H,  W]    (binary masks)\n",
    "    Returns: prototypes [2, C] (0=background, 1=foreground)\n",
    "    \"\"\"\n",
    "    # Downsample mask to feature resolution\n",
    "    mask_small = F.interpolate(\n",
    "        mask_support, size=feat_support.shape[2:], mode=\"nearest\"\n",
    "    )  # Downsamples masks to feature size via nearest neighbor\n",
    "    mask_fg = (mask_small > 0.5).float()  # [K,1,H',W']\n",
    "    mask_bg = 1.0 - mask_fg  # [K,1,H',W']\n",
    "\n",
    "    K, C, Hf, Wf = feat_support.shape\n",
    "\n",
    "    # Flatten across batch and spatial dims: [K,C,H',W'] -> [C, K*H'*W']\n",
    "    fs = feat_support.permute(1, 0, 2, 3).contiguous().view(C, -1)  # [C, K*H'*W']\n",
    "    fg_w = mask_fg.view(1, -1)  # [1, K*H'*W']\n",
    "    bg_w = mask_bg.view(1, -1)\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    # Weighted average for foreground\n",
    "    fg_proto = (fs * fg_w).sum(dim=1) / (fg_w.sum(dim=1) + eps)  # [C]\n",
    "    # Weighted average for background\n",
    "    bg_proto = (fs * bg_w).sum(dim=1) / (bg_w.sum(dim=1) + eps)  # [C]\n",
    "\n",
    "    prototypes = torch.stack([bg_proto, fg_proto], dim=0)  # [2,C]\n",
    "    return prototypes\n",
    "\n",
    "\n",
    "def classify_query(feat_query, prototypes):\n",
    "    \"\"\"\n",
    "    Classify query pixels by distance to prototypes.\n",
    "\n",
    "    feat_query: [1, C, H', W']\n",
    "    prototypes: [2, C]\n",
    "    Returns: logits [1, 2, H', W']\n",
    "    \"\"\"\n",
    "    B, C, Hq, Wq = feat_query.shape\n",
    "\n",
    "    # [1,C,H',W'] -> [H'*W', C]\n",
    "    fq = feat_query.view(C, -1).t()  # [H'*W', C]\n",
    "\n",
    "    # [2, C]\n",
    "    protos = prototypes  # [2,C]\n",
    "\n",
    "    # Compute squared Euclidean distance from each pixel to each prototype\n",
    "    # torch.cdist expects [B, N, D], so add batch dim\n",
    "    # fq_batch: [1, H'*W', C], protos_batch: [1, 2, C]\n",
    "    dists = torch.cdist(fq.unsqueeze(0), protos.unsqueeze(0))  # [1, H'*W', 2]\n",
    "    dists = dists.squeeze(0)  # [H'*W', 2]\n",
    "    dists = dists**2\n",
    "\n",
    "    # Convert distances to similarity logits: negative distance\n",
    "    logits_flat = -dists  # [H'*W', 2]\n",
    "    logits = logits_flat.t().view(1, 2, Hq, Wq)  # [1,2,H',W']\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation metric: Intersection over Union (IoU)\n",
    "\n",
    "To evaluate segmentation performance we use **Intersection over Union (IoU)**:\n",
    "\n",
    "$$\n",
    "\\text{IoU} = \\frac{\\text{intersection of predicted and true mask}}{\\text{union of predicted and true mask}}.\n",
    "$$\n",
    "\n",
    "Given the predicted logits `[1, 2, H, W]` and the ground-truth binary mask `[1, 1, H, W]`, we:\n",
    "\n",
    "1. take the `argmax` over classes to obtain a predicted mask,\n",
    "2. compute intersection and union between predicted and true rooftop pixels, and\n",
    "3. return IoU as a scalar.\n",
    "\n",
    "Metric will be used to summarise how well our few-shot model segments rooftops on held-out test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_from_logits(logits, target_mask, eps=1e-6):\n",
    "    \"\"\"\n",
    "    logits: [1,2,H,W] (class 0=background, 1=foreground)\n",
    "    target_mask: [1,1,H,W], binary 0/1\n",
    "    \"\"\"\n",
    "    # predicted class (0 or 1)\n",
    "    pred = logits.argmax(dim=1, keepdim=True).float()  # [1,1,H,W]\n",
    "    target = (target_mask > 0.5).float()\n",
    "\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Episodic meta-training\n",
    "\n",
    "We now train the encoder using episodic supervision. For each episode in the training set:\n",
    "\n",
    "1. We obtain a support image and mask and a query image and mask.\n",
    "2. We encode both images using the shared encoder.\n",
    "3. From the support features and mask, we compute foreground and background prototypes.\n",
    "4. We classify each pixel in the query feature map by its distance to these prototypes.\n",
    "5. We build the query’s ground-truth labels at the feature resolution and compute a **cross-entropy loss** between predicted logits and true classes.\n",
    "\n",
    "This process makes the encoder to learn a feature space in which simple prototype-based classification works well across many different episodes. After a few epochs, the encoder can be reused for 1-shot or K-shot segmentation on unseen test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEPENDING ON WHICH ENCODER CELL WAS RUN LAST, THIS CELL RUNS WITH OR WITHOUT SATELLITE IMAGE PRETRAINING\n",
    "### Need to keep this in mind\n",
    "\n",
    "# Freeze early layers, fine-tune deeper ones\n",
    "for name, param in encoder.named_parameters():\n",
    "    if \"layer3\" not in name and \"layer2\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "\n",
    "def meta_train(num_epochs=5):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        encoder.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for img_s, mask_s, img_q, mask_q in episode_loader:\n",
    "            # img_s: [1,K,3,H,W] – squeeze batch dim\n",
    "            img_s = img_s.squeeze(0).to(device)  # [K,3,H,W]\n",
    "            mask_s = mask_s.squeeze(0).to(device)  # [K,1,H,W]\n",
    "            img_q = img_q.to(device)\n",
    "            mask_q = mask_q.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            feat_s = encoder(img_s)  # [K,C,H',W']\n",
    "            feat_q = encoder(img_q)  # [1,C,H',W']\n",
    "\n",
    "            prototypes = compute_prototypes(feat_s, mask_s)\n",
    "            logits_q = classify_query(feat_q, prototypes)\n",
    "            logits_q = F.interpolate(logits_q, size=mask_q.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            target_q = mask_q.long().squeeze(1)\n",
    "            loss = F.cross_entropy(logits_q, target_q)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(episode_loader)\n",
    "        print(f\"Epoch {epoch}/{num_epochs} | avg episode loss: {avg_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "# Run meta-training\n",
    "meta_train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training epoch loop\n",
    "checkpoint_path = \"meta_train_checkpoint.pth\"\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": 10,\n",
    "        \"model_state_dict\": encoder.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "    },\n",
    "    checkpoint_path,\n",
    ")\n",
    "\n",
    "print(f\"Saved checkpoint to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the average episode loss**  \n",
    "The “avg episode loss” at each epoch is the average cross-entropy error over all support–query tasks in that epoch; the fact that it steadily goes from ~0.24 to ~0.09 means that the encoder is successfully learning a feature space where prototype-based segmentation works increasingly well.\n",
    "\n",
    "**Recap episodes.**  \n",
    "For one episode, we:\n",
    "\n",
    "- Take (support_image, support_mask, query_image, query_mask).\n",
    "- Encode support &rarr; compute prototypes (foreground/background).\n",
    "- Encode query &rarr; classify each pixel using the prototypes.\n",
    "- Compare predicted class vs true class at each pixel of the query.\n",
    "- Compute a cross-entropy loss over all those pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Few-shot inference: 1-shot and K-shot helpers\n",
    "\n",
    "After meta-training, comes meta-learning. We want to use the encoder for **few-shot segmentation** on new data.\n",
    "\n",
    "We define a function for that:  \n",
    "- `k_shot_predict` takes:\n",
    "  - `K` support images and masks, and\n",
    "  - a query image,\n",
    "  and returns the full-resolution logits `[1, 2, H, W]` for the query.\n",
    "- Internally, it encodes supports and query, computes prototypes using all K supports, classifies the query at the feature resolution, and upsamples the logits back to the original image size.\n",
    "\n",
    "For convenience, `one_shot_predict` wraps this function for the special case `K = 1`. These utilities are used both for quantitative evaluation and for visualising qualitative results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. Few-shot inference on test images\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def k_shot_predict(encoder, support_imgs, support_masks, query_img):\n",
    "    \"\"\"\n",
    "    K-shot segmentation for a query image given K support images+masks.\n",
    "\n",
    "    support_imgs:  [K, 3, H, W]\n",
    "    support_masks: [K, 1, H, W]\n",
    "    query_img:     [3, H, W]\n",
    "    Returns: logits [1, 2, H, W]\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        support_imgs = support_imgs.to(device)  # [K,3,H,W]\n",
    "        support_masks = support_masks.to(device)  # [K,1,H,W]\n",
    "        query_img = query_img.to(device).unsqueeze(0)  # [1,3,H,W]\n",
    "\n",
    "        # Pass through encoder\n",
    "        feat_s = encoder(support_imgs)  # [K,C,H',W']\n",
    "        feat_q = encoder(query_img)  # [1,C,H',W']\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = compute_prototypes(feat_s, support_masks)  # [2,C]\n",
    "\n",
    "        # Classify query pixels\n",
    "        logits_small = classify_query(feat_q, prototypes)  # [1,2,H',W']\n",
    "        logits = F.interpolate(\n",
    "            logits_small,\n",
    "            size=(query_img.shape[2], query_img.shape[3]),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )  # [1,2,H,W]\n",
    "\n",
    "    return logits.cpu()\n",
    "\n",
    "\n",
    "def one_shot_predict(encoder, support_img, support_mask, query_img):\n",
    "    \"\"\"\n",
    "    1-shot helper that wraps single support into K=1 form.\n",
    "    \"\"\"\n",
    "    support_imgs = support_img.unsqueeze(0)  # [1,3,H,W]\n",
    "    support_masks = support_mask.unsqueeze(0)  # [1,1,H,W]\n",
    "    return k_shot_predict(encoder, support_imgs, support_masks, query_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Results and discussion\n",
    "\n",
    "## 9.1. Quantitative evaluation: K-shot IoU\n",
    "\n",
    "To assess the model, we evaluate **K-shot IoU** on the test split:\n",
    "\n",
    "- For each test image (query), we randomly sample `K` distinct support images from the training split.\n",
    "- We run `k_shot_predict` to obtain predicted logits and compute IoU with respect to the true rooftop mask.\n",
    "- We repeat this process for multiple random test queries and report the **mean and standard deviation of IoU**.\n",
    "\n",
    "By varying `K` we can study how performance improves as we provide more labelled examples to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9a. K-shot inference on test images\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def evaluate_kshot_iou(encoder, train_dataset, test_dataset, K=5, num_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate K-shot IoU on 'num_samples' random test images.\n",
    "    For each test image, randomly sample K *distinct* support images from the train set.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # If num_samples is None, evaluate on all test samples\n",
    "    if num_samples is None:\n",
    "        num_samples = len(test_dataset)\n",
    "\n",
    "    ious = []\n",
    "    for _ in range(num_samples):\n",
    "        # pick random test index\n",
    "        ti = rng.integers(0, len(test_dataset))\n",
    "        img_q, mask_q = test_dataset[ti]\n",
    "\n",
    "        # pick K distinct support indices\n",
    "        support_indices = rng.choice(len(train_dataset), size=K, replace=False)\n",
    "        support_imgs = []\n",
    "        support_masks = []\n",
    "        for si in support_indices:\n",
    "            img_s, mask_s = train_dataset[si]\n",
    "            support_imgs.append(img_s)\n",
    "            support_masks.append(mask_s)\n",
    "        support_imgs = torch.stack(support_imgs, dim=0)  # [K,3,H,W]\n",
    "        support_masks = torch.stack(support_masks, dim=0)  # [K,1,H,W]\n",
    "\n",
    "        # run K-shot prediction\n",
    "        logits = k_shot_predict(encoder, support_imgs, support_masks, img_q)  # [1,2,H,W]\n",
    "\n",
    "        iou = iou_from_logits(logits, mask_q.unsqueeze(0))\n",
    "        ious.append(iou)\n",
    "\n",
    "    ious = np.array(ious)\n",
    "    print(f\"{K}-shot mean IoU over {num_samples} test samples: {ious.mean():.3f} ± {ious.std():.3f}\")\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Visualising a 1-shot episode\n",
    "\n",
    "1. Randomly sample a support image and mask from the training set.\n",
    "2. Randomly sample a query image and mask from the test set.\n",
    "3. Run `one_shot_predict` to obtain the predicted query mask.\n",
    "4. Plot:\n",
    "   - the support image and its mask,\n",
    "   - the query image and its ground-truth mask,\n",
    "   - and the predicted mask.\n",
    "\n",
    "This helps build intuition about where the model succeeds or fails—for example, whether it captures rooftop shapes well and where it confuses rooftops with other bright surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_rgb(img_tensor):\n",
    "    \"\"\"Undo normalisation and convert [3,H,W] tensor to [H,W,3] RGB numpy.\"\"\"\n",
    "    img_np = img_tensor.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    return img_np\n",
    "\n",
    "\n",
    "def visualise_few_shot_example(encoder, train_dataset, test_dataset):\n",
    "    encoder.eval()\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # pick support from train, query from test\n",
    "    si = rng.integers(0, len(train_dataset))\n",
    "    ti = rng.integers(0, len(test_dataset))\n",
    "\n",
    "    img_s, mask_s = train_dataset[si]\n",
    "    img_q, mask_q = test_dataset[ti]\n",
    "\n",
    "    logits = one_shot_predict(encoder, img_s, mask_s, img_q)  # [1,2,H,W]\n",
    "    pred_mask = logits.argmax(dim=1, keepdim=True).float().squeeze(0).squeeze(0).numpy()\n",
    "\n",
    "    # convert to numpy for plotting\n",
    "    img_s_np = tensor_to_rgb(img_s)\n",
    "    img_q_np = tensor_to_rgb(img_q)\n",
    "    mask_s_np = mask_s.squeeze(0).numpy()\n",
    "    mask_q_np = mask_q.squeeze(0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(img_s_np)\n",
    "    plt.title(\"Support image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(mask_s_np, cmap=\"gray\")\n",
    "    plt.title(\"Support mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.imshow(img_q_np)\n",
    "    plt.title(\"Query image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(mask_q_np, cmap=\"gray\")\n",
    "    plt.title(\"Query GT mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(pred_mask, cmap=\"gray\")\n",
    "    plt.title(\"Predicted mask (1-shot)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualise_few_shot_example(encoder, train_base, test_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Visualising a K-shot episode\n",
    "\n",
    "Finally, we visualise a **K-shot** episode (e.g. 5-shot) to complement the numerical results:\n",
    "\n",
    "1. Select a random query image from the test set.\n",
    "2. Select `K` random support images from the training set.\n",
    "3. Run `k_shot_predict` to produce a predicted mask.\n",
    "4. Plot:\n",
    "   - the query image,\n",
    "   - the ground-truth query mask,\n",
    "   - and the K-shot predicted mask.\n",
    "\n",
    "Qualitatively comparing this to the 1-shot visualisation helps us see where additional support images improve the segmentation—typically in challenging cases such as partially occluded rooftops or unusual roof materials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_kshot_example(encoder, train_dataset, test_dataset, K=5):\n",
    "    encoder.eval()\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # pick query from test\n",
    "    ti = rng.integers(0, len(test_dataset))\n",
    "    img_q, mask_q = test_dataset[ti]\n",
    "\n",
    "    # pick K supports from train\n",
    "    support_indices = rng.choice(len(train_dataset), size=K, replace=False)\n",
    "    support_imgs, support_masks = [], []\n",
    "    for si in support_indices:\n",
    "        img_s, mask_s = train_dataset[si]\n",
    "        support_imgs.append(img_s)\n",
    "        support_masks.append(mask_s)\n",
    "    support_imgs = torch.stack(support_imgs, dim=0)  # [K,3,H,W]\n",
    "    support_masks = torch.stack(support_masks, dim=0)  # [K,1,H,W]\n",
    "\n",
    "    # prediction\n",
    "    logits = k_shot_predict(encoder, support_imgs, support_masks, img_q)\n",
    "    pred_mask = logits.argmax(dim=1, keepdim=True).float().squeeze().numpy()\n",
    "\n",
    "    img_q_np = tensor_to_rgb(img_q)\n",
    "    mask_q_np = mask_q.squeeze(0).numpy()\n",
    "\n",
    "    # Plot\n",
    "    cols = max(K, 3)\n",
    "    plt.figure(figsize=(4 * cols, 8))\n",
    "\n",
    "    # first row: support images\n",
    "    for i in range(K):\n",
    "        plt.subplot(2, cols, i + 1)\n",
    "        plt.imshow(tensor_to_rgb(support_imgs[i]))\n",
    "        plt.title(f\"Support {i+1} image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    # second row: support masks\n",
    "    for i in range(K):\n",
    "        plt.subplot(2, cols, cols + i + 1)\n",
    "        plt.imshow(support_masks[i].squeeze(0).numpy(), cmap=\"gray\")\n",
    "        plt.title(f\"Support {i+1} mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    # query image & masks to the right (replace last columns if needed)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img_q_np)\n",
    "    plt.title(\"Query image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask_q_np, cmap=\"gray\")\n",
    "    plt.title(\"Query GT mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred_mask, cmap=\"gray\")\n",
    "    plt.title(f\"Predicted mask ({K}-shot)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example 5-shot visualisation\n",
    "visualise_kshot_example(encoder, train_base, test_base, K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. Comparing 1-shot and 5-shot performance\n",
    "\n",
    "Using the evaluation function above, we can now compare:\n",
    "\n",
    "- **1-shot IoU**, where only a single annotated support image is available, and\n",
    "- **5-shot IoU**, where we provide five labelled support images for each test query.\n",
    "\n",
    "We expect performance to improve with more support examples, as the prototypes become more representative of the diversity of rooftop appearances. This experiment illustrates a key trade-off in few-shot learning between **annotation cost** (how many labelled examples we need) and **model performance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9d. K-shot inference on test images\n",
    "# ============================================================\n",
    "\n",
    "# 1-shot using the same function\n",
    "ious_1shot = evaluate_kshot_iou(encoder, train_base, test_base, K=1, num_samples=None)\n",
    "\n",
    "# 5-shot\n",
    "ious_5shot = evaluate_kshot_iou(encoder, train_base, test_base, K=5, num_samples=None)\n",
    "\n",
    "# 10-shot\n",
    "ious_10shot = evaluate_kshot_iou(encoder, train_base, test_base, K=10, num_samples=None)\n",
    "\n",
    "# 20-shot\n",
    "ious_20shot = evaluate_kshot_iou(encoder, train_base, test_base, K=20, num_samples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPLETE HERE\n",
    "Ideas:\n",
    "- difference normal ResNet and pretrained ResNet\n",
    "- the slow performance increase with higher k \n",
    "- general implications of few-shot for our usecase: are other methods more suitable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alsentzer, E., Li, M. M., Kobren, S. N., Noori, A., Undiagnosed Diseases Network, Kohane, I. S., & Zitnik, M. (2025). *Few shot learning for phenotype-driven diagnosis of patients with rare genetic diseases*. **npj Digital Medicine, 8**(1), 380. https://doi.org/10.1038/s41746-025-01749-1\n",
    "\n",
    "- Castello, R., Walch, A., Attias, R., Cadei, R., Jiang, S., & Scartezzini, J.-L. (2021). *Quantification of the suitable rooftop area for solar panel installation from overhead imagery using convolutional neural networks*. **Journal of Physics: Conference Series, 2042**(1), 012002. https://doi.org/10.1088/1742-6596/2042/1/012002\n",
    "\n",
    "- Chen, Y., Wei, C., Wang, D., Ji, C., & Li, B. (2022). *Semi-supervised contrastive learning for few-shot segmentation of remote sensing images*. **Remote Sensing, 14**(17), 4254. https://doi.org/10.3390/rs14174254\n",
    "\n",
    "- Finn, C., Abbeel, P., & Levine, S. (2017). *Model-agnostic meta-learning for fast adaptation of deep networks*. In **International Conference on Machine Learning** (pp. 1126–1135). PMLR. https://doi.org/10.48550/arXiv.1703.03400\n",
    "\n",
    "- Ge, Z., Fan, X., Zhang, J., & Jin, S. (2025). *SegPPD-FS: Segmenting plant pests and diseases in the wild using few-shot learning*. **Plant Phenomics**, 100121. https://doi.org/10.1016/j.plaphe.2025.100121\n",
    "\n",
    "- Hu, Y., Liu, C., Li, Z., Xu, J., Han, Z., & Guo, J. (2022). *Few-shot building footprint shape classification with relation network*. **ISPRS International Journal of Geo-Information, 11**(5), 311. https://doi.org/10.3390/ijgi11050311\n",
    "\n",
    "- Jadon, S. (2021, February). *COVID-19 detection from scarce chest x-ray image data using few-shot deep learning approach*. In **Medical Imaging 2021: Imaging Informatics for Healthcare, Research, and Applications** (Vol. 11601, pp. 161–170). SPIE. https://doi.org/10.1117/12.2581496\n",
    "\n",
    "- Lee, G. Y., Dam, T., Ferdaus, M. M., Poenar, D. P., & Duong, V. (2025). *Enhancing Few-Shot Classification of Benchmark and Disaster Imagery with ATTBHFA-Net*. **arXiv preprint arXiv:2510.18326**. https://doi.org/10.48550/arXiv.2510.18326\n",
    "\n",
    "- Li, X., He, Z., Zhang, L., Guo, S., Hu, B., & Guo, K. (2025). *CDCNet: Cross-domain few-shot learning with adaptive representation enhancement*. **Pattern Recognition, 162**, 111382. https://doi.org/10.1016/j.patcog.2025.111382\n",
    "\n",
    "- Puthumanaillam, G., & Verma, U. (2023). *Texture based prototypical network for few-shot semantic segmentation of forest cover: Generalizing for different geographical regions*. **Neurocomputing, 538**, 126201. https://doi.org/10.1016/j.neucom.2023.03.062\n",
    "\n",
    "- Snell, J., Swersky, K., & Zemel, R. (2017). *Prototypical networks for few-shot learning*. **Advances in Neural Information Processing Systems, 30**. https://doi.org/10.48550/arXiv.1703.05175\n",
    "\n",
    "- Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P. H., & Hospedales, T. M. (2018). *Learning to compare: Relation network for few-shot learning*. In **Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition** (pp. 1199–1208). https://doi.org/10.1109/CVPR.2018.00131\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
